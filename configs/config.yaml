#Represents a 2 layer network. 3072 is the input layer
layer_specs: [3072, 128, 10]  

# Type of non-linear activation function.
activation: "ReLU"

# Number of training samples per batch to be passed to network
batch_size: 128

# Number of epochs to train the model
epochs: 100

# The learning rate to be used for training.
learning_rate: 0.004

# Flag to enable early stopping
early_stop: True

# History for early stopping. Wait for this many epochs to check validation loss / accuracy.
early_stop_epoch: 5

#regularization Type: 1 for L1, 2 for L2
regularization_type: 1

# Regularization constant
L2_penalty: 0.0001

# Use momentum for training
momentum: True

# Value for the parameter 'gamma' in momentum
momentum_gamma: 0.9



